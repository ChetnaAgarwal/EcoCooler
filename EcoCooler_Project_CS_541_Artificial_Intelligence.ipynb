{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0PsKLImGFEq"
      },
      "source": [
        "#### PROBLEM STATEMENT: Minimizing Costs in Energy Consumption of a Data Center (by Chetna Agarwal)\n",
        "In this case study, the primary objective is to minimize energy consumption costs in the cooling and heating systems of a data center. Data centers, known for their high energy demands, rely heavily on cooling systems to maintain optimal operating temperatures for servers and equipment. Inspired by DeepMind's achievement in reducing Google's data center cooling costs by 40% using a Deep Q-Learning (DQN) AI model in 2016, this case study aims to replicate a similar approach.\n",
        "\n",
        "\n",
        "- Developed an AI model using Deep Q-Learning to optimize server temperature regulation, achieving 87% energy savings.\n",
        "- Designed environment simulation methods for real-time AI action updates, current state retrieval, and reward calculation.\n",
        "- Built neural network architecture with fully connected input, hidden, and output layers; optimized with Mean Squared Error loss and Adam optimizer.\n",
        "- Implemented Experience Replay in DQN model to enhance learning efficiency with batch inputs and targets.\n",
        "- Trained AI through Deep Q-Learning with exploration-exploitation strategy, achieving optimal cooling/heating predictions with minimal energy use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzx2JRnfWV66"
      },
      "source": [
        "### Build the Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndqjgsQ6S3ZI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# BUILD THE ENVIRONMENT IN A CLASS\n",
        "\n",
        "class Environment(object):\n",
        "\n",
        "    # INTRODUCE AND INITIALIZE ALL THE PARAMETERS AND VARIABLES OF THE ENVIRONMENT\n",
        "\n",
        "    def __init__(self, optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 10, initial_rate_data = 60):\n",
        "        self.monthly_atmospheric_temperatures = [1.0, 5.0, 7.0, 10.0, 11.0, 20.0, 23.0, 24.0, 22.0, 10.0, 5.0, 1.0] # list containing the average monthly atmospheric temperatures for each of the 12 months\n",
        "        self.initial_month = initial_month\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[initial_month] # conatins average atmospheric temperature of the month we are in during the simulation\n",
        "        self.optimal_temperature = optimal_temperature\n",
        "        self.min_temperature = -20\n",
        "        self.max_temperature = 80\n",
        "        self.min_number_users = 10\n",
        "        self.max_number_users = 100\n",
        "        self.max_update_users = 5\n",
        "        self.min_rate_data = 20\n",
        "        self.max_rate_data = 300\n",
        "        self.max_update_data = 10\n",
        "        self.initial_number_users = initial_number_users\n",
        "        self.current_number_users = initial_number_users\n",
        "        self.initial_rate_data = initial_rate_data\n",
        "        self.current_rate_data = initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0 # tells the AI whether or not we should reset the temperature of the server, in case it goes outside the allowed range of [-20, 80]. if reset, it will be set to 1, otherwise remain at 0.\n",
        "        self.train = 1 # tells whether we are in training and inference mode (training = 1, inference = 0)\n",
        "\n",
        "    # UPDATES THE ENVIRONMENT RIGHT AFTER THE AI PLAYS AN ACTION\n",
        "    def update_env(self, direction, energy_ai, month):\n",
        "        # 1. direction: describes the direction of heat transfer the AI imposes on the server.\n",
        "        # if direction = 1, the AI is heating up the server\n",
        "        # if direction = -1, the AI is cooling down the server\n",
        "        # 2. energy_ai: the energy spent by AI to heat up or cool down the server at the specific time when action is played. (equal to temp change caused in server)\n",
        "\n",
        "        # COMPUTE THE REWARD\n",
        "\n",
        "        # Computing the energy spent by the server's cooling system when there is no AI\n",
        "        energy_noai = 0\n",
        "        if (self.temperature_noai < self.optimal_temperature[0]):\n",
        "            energy_noai = self.optimal_temperature[0] - self.temperature_noai\n",
        "            self.temperature_noai = self.optimal_temperature[0]\n",
        "        elif (self.temperature_noai > self.optimal_temperature[1]):\n",
        "            energy_noai = self.temperature_noai - self.optimal_temperature[1]\n",
        "            self.temperature_noai = self.optimal_temperature[1]\n",
        "        # Computing the Reward\n",
        "        self.reward = energy_noai - energy_ai\n",
        "        # Scaling the Reward - normalization is recommended in many research papers when doing deep RL as it stabilizes the training and improves the performance.\n",
        "        self.reward = 1e-3 * self.reward\n",
        "\n",
        "        # GETTING THE NEXT STATE\n",
        "        # Each state is composed of the elements: (1) temp of server at time t, (2) number of users in server at time t, (3) rate of data transmission in the server at time t\n",
        "\n",
        "        # Updating the atmospheric temperature\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[month]\n",
        "        # Updating the number of users\n",
        "        self.current_number_users += np.random.randint(-self.max_update_users, self.max_update_users)\n",
        "        if (self.current_number_users > self.max_number_users):\n",
        "            self.current_number_users = self.max_number_users\n",
        "        elif (self.current_number_users < self.min_number_users):\n",
        "            self.current_number_users = self.min_number_users\n",
        "        # Updating the rate of data\n",
        "        self.current_rate_data += np.random.randint(-self.max_update_data, self.max_update_data)\n",
        "        if (self.current_rate_data > self.max_rate_data):\n",
        "            self.current_rate_data = self.max_rate_data\n",
        "        elif (self.current_rate_data < self.min_rate_data):\n",
        "            self.current_rate_data = self.min_rate_data\n",
        "        # Computing the Delta of Intrinsic Temperature\n",
        "        past_intrinsic_temperature = self.intrinsic_temperature\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        delta_intrinsic_temperature = self.intrinsic_temperature - past_intrinsic_temperature\n",
        "        # Computing the Delta of Temperature caused by the AI\n",
        "        if (direction == -1):\n",
        "            delta_temperature_ai = -energy_ai\n",
        "        elif (direction == 1):\n",
        "            delta_temperature_ai = energy_ai\n",
        "        # Updating the new Server's Temperature when there is the AI\n",
        "        self.temperature_ai += delta_intrinsic_temperature + delta_temperature_ai\n",
        "        # Updating the new Server's Temperature when there is no AI\n",
        "        self.temperature_noai += delta_intrinsic_temperature\n",
        "\n",
        "        # GETTING GAME OVER\n",
        "        # Update game_over if needed (ie, if temp of server goes outside the allowed range)\n",
        "\n",
        "        if (self.temperature_ai < self.min_temperature):\n",
        "            if (self.train == 1):\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.total_energy_ai += self.optimal_temperature[0] - self.temperature_ai\n",
        "                self.temperature_ai = self.optimal_temperature[0]\n",
        "        elif (self.temperature_ai > self.max_temperature):\n",
        "            if (self.train == 1):\n",
        "                self.game_over = 1\n",
        "            else:\n",
        "                self.total_energy_ai += self.temperature_ai - self.optimal_temperature[1]\n",
        "                self.temperature_ai = self.optimal_temperature[1]\n",
        "\n",
        "        # UPDATING THE SCORES coming from 2 separate simulations (with AI and without AI)\n",
        "\n",
        "        # Updating the Total Energy spent by the AI\n",
        "        self.total_energy_ai += energy_ai\n",
        "        # Updating the Total Energy spent by the server's cooling system when there is no AI\n",
        "        self.total_energy_noai += energy_noai\n",
        "\n",
        "        # SCALING THE NEXT STATE\n",
        "        # scale the next state by scaling each of its components (to improve performance): server temp, num of users, data transmission rate\n",
        "\n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users) / (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n",
        "        next_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
        "\n",
        "        # RETURNING THE NEXT STATE, THE REWARD, AND GAME OVER\n",
        "\n",
        "        return next_state, self.reward, self.game_over\n",
        "\n",
        "    # MAKING A METHOD THAT RESETS THE ENVIRONMENT when a new training episode starts, by resetting all variables to their originally initialized values\n",
        "\n",
        "    def reset(self, new_month):\n",
        "        self.atmospheric_temperature = self.monthly_atmospheric_temperatures[new_month]\n",
        "        self.initial_month = new_month\n",
        "        self.current_number_users = self.initial_number_users\n",
        "        self.current_rate_data = self.initial_rate_data\n",
        "        self.intrinsic_temperature = self.atmospheric_temperature + 1.25 * self.current_number_users + 1.25 * self.current_rate_data\n",
        "        self.temperature_ai = self.intrinsic_temperature\n",
        "        self.temperature_noai = (self.optimal_temperature[0] + self.optimal_temperature[1]) / 2.0\n",
        "        self.total_energy_ai = 0.0\n",
        "        self.total_energy_noai = 0.0\n",
        "        self.reward = 0.0\n",
        "        self.game_over = 0\n",
        "        self.train = 1\n",
        "\n",
        "    # MAKING A METHOD THAT GIVES US AT ANY TIME THE CURRENT STATE, THE LAST REWARD AND WHETHER THE GAME IS OVER\n",
        "\n",
        "    def observe(self):\n",
        "        scaled_temperature_ai = (self.temperature_ai - self.min_temperature) / (self.max_temperature - self.min_temperature)\n",
        "        scaled_number_users = (self.current_number_users - self.min_number_users) / (self.max_number_users - self.min_number_users)\n",
        "        scaled_rate_data = (self.current_rate_data - self.min_rate_data) / (self.max_rate_data - self.min_rate_data)\n",
        "        current_state = np.matrix([scaled_temperature_ai, scaled_number_users, scaled_rate_data])\n",
        "        return current_state, self.reward, self.game_over"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7tuuk1jWc6n"
      },
      "source": [
        "Build the Brain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4-Tn0VkfWtI-",
        "outputId": "3aa74ad5-3891-4aa0-dfdb-20f8941c7b57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "# Build the Brain with Dropout\n",
        "\n",
        "# Importing the libraries\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "# BUILDING THE BRAIN\n",
        "\n",
        "class Brain(object):\n",
        "\n",
        "    # BUILDING A FULLY CONNECTED NEURAL NETWORK DIRECTLY INSIDE THE INIT METHOD\n",
        "\n",
        "    def __init__(self, learning_rate = 0.001, number_actions = 5):\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # BUILDIND THE INPUT LAYER COMPOSED OF THE INPUT STATE\n",
        "        states = Input(shape = (3,))\n",
        "\n",
        "        # BUILDING THE FIRST FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\n",
        "        x = Dense(units = 64, activation = 'sigmoid')(states)\n",
        "        x = Dropout(rate = 0.1)(x)\n",
        "\n",
        "        # BUILDING THE SECOND FULLY CONNECTED HIDDEN LAYER WITH DROPOUT ACTIVATED\n",
        "        y = Dense(units = 32, activation = 'sigmoid')(x)\n",
        "        y = Dropout(rate = 0.1)(y)\n",
        "\n",
        "        # BUILDING THE OUTPUT LAYER, FULLY CONNECTED TO THE LAST HIDDEN LAYER\n",
        "        q_values = Dense(units = number_actions, activation = 'softmax')(y)\n",
        "\n",
        "        # ASSEMBLING THE FULL ARCHITECTURE INSIDE A MODEL OBJECT\n",
        "        self.model = Model(inputs = states, outputs = q_values)\n",
        "\n",
        "        # COMPILING THE MODEL WITH A MEAN-SQUARED ERROR LOSS AND A CHOSEN OPTIMIZER\n",
        "        self.model.compile(loss = 'mse', optimizer = Adam(lr = learning_rate))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nAQsIauXQac"
      },
      "source": [
        "Implement Deep Q-Learning with Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FquxIj4Xfv0"
      },
      "outputs": [],
      "source": [
        "# Importing the libraries\n",
        "import numpy as np\n",
        "\n",
        "# IMPLEMENTING DEEP Q-LEARNING WITH EXPERIENCE REPLAY\n",
        "\n",
        "class DQN(object):\n",
        "\n",
        "    # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN\n",
        "    def __init__(self, max_memory = 100, discount = 0.9):\n",
        "        self.memory = list()\n",
        "        self.max_memory = max_memory\n",
        "        self.discount = discount\n",
        "\n",
        "    # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY\n",
        "    def remember(self, transition, game_over):\n",
        "        self.memory.append([transition, game_over])\n",
        "        if len(self.memory) > self.max_memory:\n",
        "            del self.memory[0]\n",
        "\n",
        "    # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY\n",
        "    def get_batch(self, model, batch_size = 10):\n",
        "        len_memory = len(self.memory)\n",
        "        num_inputs = self.memory[0][0][0].shape[1]\n",
        "        num_outputs = model.output_shape[-1]\n",
        "        inputs = np.zeros((min(len_memory, batch_size), num_inputs))\n",
        "        targets = np.zeros((min(len_memory, batch_size), num_outputs))\n",
        "        for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):\n",
        "            current_state, action, reward, next_state = self.memory[idx][0]\n",
        "            game_over = self.memory[idx][1]\n",
        "            inputs[i] = current_state\n",
        "            targets[i] = model.predict(current_state)[0]\n",
        "            Q_sa = np.max(model.predict(next_state)[0])\n",
        "            if game_over:\n",
        "                targets[i, action] = reward\n",
        "            else:\n",
        "                targets[i, action] = reward + self.discount * Q_sa\n",
        "        return inputs, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E17pd1rXnAQ"
      },
      "source": [
        "Train the AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1632
        },
        "id": "aOhACytXXvVD",
        "outputId": "0fc97721-2463-48e2-ab2f-4ac13aa211cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0616 13:11:18.977390 140157426132864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0616 13:11:19.026131 140157426132864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0616 13:11:19.033931 140157426132864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0616 13:11:19.054827 140157426132864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0616 13:11:19.065196 140157426132864 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0616 13:11:19.139321 140157426132864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0616 13:11:19.158454 140157426132864 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Epoch: 001/100\n",
            "Total Energy spent with an AI: 30\n",
            "Total Energy spent with no AI: 146\n",
            "\n",
            "\n",
            "Epoch: 002/100\n",
            "Total Energy spent with an AI: 0\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 003/100\n",
            "Total Energy spent with an AI: 4\n",
            "Total Energy spent with no AI: 22\n",
            "\n",
            "\n",
            "Epoch: 004/100\n",
            "Total Energy spent with an AI: 28\n",
            "Total Energy spent with no AI: 116\n",
            "\n",
            "\n",
            "Epoch: 005/100\n",
            "Total Energy spent with an AI: 46\n",
            "Total Energy spent with no AI: 224\n",
            "\n",
            "\n",
            "Epoch: 006/100\n",
            "Total Energy spent with an AI: 16\n",
            "Total Energy spent with no AI: 113\n",
            "\n",
            "\n",
            "Epoch: 007/100\n",
            "Total Energy spent with an AI: 27\n",
            "Total Energy spent with no AI: 105\n",
            "\n",
            "\n",
            "Epoch: 008/100\n",
            "Total Energy spent with an AI: 28\n",
            "Total Energy spent with no AI: 122\n",
            "\n",
            "\n",
            "Epoch: 009/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 010/100\n",
            "Total Energy spent with an AI: 14\n",
            "Total Energy spent with no AI: 105\n",
            "\n",
            "\n",
            "Epoch: 011/100\n",
            "Total Energy spent with an AI: 0\n",
            "Total Energy spent with no AI: 7\n",
            "\n",
            "\n",
            "Epoch: 012/100\n",
            "Total Energy spent with an AI: 0\n",
            "Total Energy spent with no AI: 2\n",
            "\n",
            "\n",
            "Epoch: 013/100\n",
            "Total Energy spent with an AI: 3\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 014/100\n",
            "Total Energy spent with an AI: 0\n",
            "Total Energy spent with no AI: 0\n",
            "\n",
            "\n",
            "Epoch: 015/100\n",
            "Total Energy spent with an AI: 0\n",
            "Total Energy spent with no AI: 0\n",
            "Early Stopping\n"
          ]
        }
      ],
      "source": [
        "# Training the AI with Early Stopping\n",
        "\n",
        "# Importing the libraries and the other python files\n",
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)\n",
        "\n",
        "# SETTING THE PARAMETERS\n",
        "epsilon = .3\n",
        "number_actions = 5\n",
        "direction_boundary = (number_actions - 1) / 2\n",
        "number_epochs = 100\n",
        "max_memory = 3000\n",
        "batch_size = 512\n",
        "temperature_step = 1.5\n",
        "\n",
        "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
        "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
        "\n",
        "# BUILDING THE BRAIN BY SIMPLY CREATING AN OBJECT OF THE BRAIN CLASS\n",
        "brain = Brain(learning_rate = 0.00001, number_actions = number_actions)\n",
        "\n",
        "# BUILDING THE DQN MODEL BY SIMPLY CREATING AN OBJECT OF THE DQN CLASS\n",
        "dqn = DQN(max_memory = max_memory, discount = 0.9)\n",
        "\n",
        "# CHOOSING THE MODE\n",
        "train = True\n",
        "\n",
        "# TRAINING THE AI\n",
        "env.train = train\n",
        "model = brain.model\n",
        "early_stopping = True\n",
        "patience = 10\n",
        "best_total_reward = -np.inf\n",
        "patience_count = 0\n",
        "if (env.train):\n",
        "    # STARTING THE LOOP OVER ALL THE EPOCHS (1 Epoch = 5 Months)\n",
        "    for epoch in range(1, number_epochs):\n",
        "        # INITIALIAZING ALL THE VARIABLES OF BOTH THE ENVIRONMENT AND THE TRAINING LOOP\n",
        "        total_reward = 0\n",
        "        loss = 0.\n",
        "        new_month = np.random.randint(0, 12)\n",
        "        env.reset(new_month = new_month)\n",
        "        game_over = False\n",
        "        current_state, _, _ = env.observe()\n",
        "        timestep = 0\n",
        "        # STARTING THE LOOP OVER ALL THE TIMESTEPS (1 Timestep = 1 Minute) IN ONE EPOCH\n",
        "        while ((not game_over) and timestep <= 5 * 30 * 24 * 60):\n",
        "            # PLAYING THE NEXT ACTION BY EXPLORATION\n",
        "            if np.random.rand() <= epsilon:\n",
        "                action = np.random.randint(0, number_actions)\n",
        "                if (action - direction_boundary < 0):\n",
        "                    direction = -1\n",
        "                else:\n",
        "                    direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "            # PLAYING THE NEXT ACTION BY INFERENCE\n",
        "            else:\n",
        "                q_values = model.predict(current_state)\n",
        "                action = np.argmax(q_values[0])\n",
        "                if (action - direction_boundary < 0):\n",
        "                    direction = -1\n",
        "                else:\n",
        "                    direction = 1\n",
        "                energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "            # UPDATING THE ENVIRONMENT AND REACHING THE NEXT STATE\n",
        "            next_state, reward, game_over = env.update_env(direction, energy_ai, ( new_month + int(timestep/(30*24*60)) ) % 12)\n",
        "            total_reward += reward\n",
        "            # STORING THIS NEW TRANSITION INTO THE MEMORY\n",
        "            dqn.remember([current_state, action, reward, next_state], game_over)\n",
        "            # GATHERING IN TWO SEPARATE BATCHES THE INPUTS AND THE TARGETS\n",
        "            inputs, targets = dqn.get_batch(model, batch_size = batch_size)\n",
        "            # COMPUTING THE LOSS OVER THE TWO WHOLE BATCHES OF INPUTS AND TARGETS\n",
        "            loss += model.train_on_batch(inputs, targets)\n",
        "            timestep += 1\n",
        "            current_state = next_state\n",
        "        # PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
        "        print(\"\\n\")\n",
        "        print(\"Epoch: {:03d}/{:03d}\".format(epoch, number_epochs))\n",
        "        print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
        "        print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
        "        # EARLY STOPPING\n",
        "        if (early_stopping):\n",
        "            if (total_reward <= best_total_reward):\n",
        "                patience_count += 1\n",
        "            elif (total_reward > best_total_reward):\n",
        "                best_total_reward = total_reward\n",
        "                patience_count = 0\n",
        "            if (patience_count >= patience):\n",
        "                print(\"Early Stopping\")\n",
        "                break\n",
        "        # SAVING THE MODEL\n",
        "        model.save(\"model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IogSe-VX6q4"
      },
      "source": [
        "# Step 5: Testing the AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "DAPXJzSaX-dH",
        "outputId": "fa4b0115-544e-4a99-ec94-f33074f1c46a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Total Energy spent with an AI: 261985\n",
            "Total Energy spent with no AI: 1978293\n",
            "ENERGY SAVED: 87 %\n"
          ]
        }
      ],
      "source": [
        "# Testing the AI\n",
        "\n",
        "# Installing Keras\n",
        "# conda install -c conda-forge keras\n",
        "\n",
        "# Importing the libraries and the other python files\n",
        "import os\n",
        "import numpy as np\n",
        "import random as rn\n",
        "from keras.models import load_model\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(42)\n",
        "rn.seed(12345)\n",
        "\n",
        "# SETTING THE PARAMETERS\n",
        "number_actions = 5\n",
        "direction_boundary = (number_actions - 1) / 2\n",
        "temperature_step = 1.5\n",
        "\n",
        "# BUILDING THE ENVIRONMENT BY SIMPLY CREATING AN OBJECT OF THE ENVIRONMENT CLASS\n",
        "env = Environment(optimal_temperature = (18.0, 24.0), initial_month = 0, initial_number_users = 20, initial_rate_data = 30)\n",
        "\n",
        "# LOADING A PRE-TRAINED BRAIN\n",
        "model = load_model(\"model.h5\")\n",
        "\n",
        "# CHOOSING THE MODE\n",
        "train = False\n",
        "\n",
        "# RUNNING A 1 YEAR SIMULATION IN INFERENCE MODE\n",
        "env.train = train\n",
        "current_state, _, _ = env.observe()\n",
        "for timestep in range(0, 12 * 30 * 24 * 60):\n",
        "    q_values = model.predict(current_state)\n",
        "    action = np.argmax(q_values[0])\n",
        "    if (action - direction_boundary < 0):\n",
        "        direction = -1\n",
        "    else:\n",
        "        direction = 1\n",
        "    energy_ai = abs(action - direction_boundary) * temperature_step\n",
        "    next_state, reward, game_over = env.update_env(direction, energy_ai, int(timestep / (30 * 24 * 60)))\n",
        "    current_state = next_state\n",
        "\n",
        "# PRINTING THE TRAINING RESULTS FOR EACH EPOCH\n",
        "print(\"\\n\")\n",
        "print(\"Total Energy spent with an AI: {:.0f}\".format(env.total_energy_ai))\n",
        "print(\"Total Energy spent with no AI: {:.0f}\".format(env.total_energy_noai))\n",
        "print(\"ENERGY SAVED: {:.0f} %\".format((env.total_energy_noai - env.total_energy_ai) / env.total_energy_noai * 100))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
